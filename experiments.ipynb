{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMtQXKks4zj3cyfnMxQTJmS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/scrannah/dnnls_final_project/blob/master/experiments.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rm -rf /content/dnnls_final_project\n"
      ],
      "metadata": {
        "id": "5tIaBJjvVER9"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6jLHj0ncUz3v",
        "outputId": "3b1b8072-3785-49c4-d062-f05ac10894d3"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!git clone https://github.com/scrannah/dnnls_final_project.git\n",
        "%cd /content/dnnls_final_project\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IFHu0dtz_Z81",
        "outputId": "4d7917a0-e024-41cd-b461-83908b429ab6"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'dnnls_final_project'...\n",
            "remote: Enumerating objects: 249, done.\u001b[K\n",
            "remote: Counting objects: 100% (249/249), done.\u001b[K\n",
            "remote: Compressing objects: 100% (171/171), done.\u001b[K\n",
            "remote: Total 249 (delta 140), reused 173 (delta 67), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (249/249), 34.36 KiB | 8.59 MiB/s, done.\n",
            "Resolving deltas: 100% (140/140), done.\n",
            "/content/dnnls_final_project\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "gZzbOwYQUYM_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed4bed69-e51f-42af-b588-68bc38973788"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Importing the necessary libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "from datasets.fingerprint import random\n",
        "from torch.utils.data import Dataset, DataLoader, random_split, Subset\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision.transforms.functional as FT\n",
        "import torch.nn.functional as F\n",
        "from torchmetrics.image import StructuralSimilarityIndexMeasure\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "\n",
        "from transformers import BertTokenizer\n",
        "import gc\n",
        "\n",
        "import textwrap\n",
        "import sys\n",
        "import os\n",
        "\n",
        "abspath = r\".\\dnnls_final_project\"\n",
        "sys.path.append(os.path.abspath(abspath ))\n",
        "\n",
        "# put src imports here models training utils etc\n",
        "from src.attention.attention import Attention\n",
        "\n",
        "from src.dataloaders.sp_dataset import SequencePredictionDataset\n",
        "from src.dataloaders.tag_extraction import parse_gdi_text\n",
        "from src.dataloaders.text_dataset import TextTaskDataset\n",
        "from src.dataloaders.vae_dataset import AutoEncoderTaskDataset\n",
        "\n",
        "from src.encoders.visual_autoencoder import Backbone, VisualEncoder, VisualDecoder, VisualAutoencoder\n",
        "from src.encoders.text_autoencoder import EncoderLSTM, DecoderLSTM, Seq2SeqLSTM\n",
        "\n",
        "from src.model.sequence_predictor import SequencePredictor\n",
        "\n",
        "from src.train.train_sequence_predictor import train_sequence_predictor\n",
        "from src.train.train_visual_autoencoder import train_visual_autoencoder\n",
        "\n",
        "from src.utils.checkpoints import save_checkpoint_to_drive, load_checkpoint_from_drive\n",
        "from src.utils.training_utils import init_weights, validation\n",
        "from src.utils.token_generate import generate\n"
      ],
      "metadata": {
        "id": "SGQbQHWcEwVg"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "att = Attention(12)\n",
        "print(att) # test to see if attention imports"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V3xwmRDaiHN6",
        "outputId": "a3ed0391-4ab7-448b-f1fc-6dceefa9fe07"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention(\n",
            "  (attn): Linear(in_features=12, out_features=1, bias=True)\n",
            "  (softmax): Softmax(dim=1)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Variables and initial setup\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "N_EPOCHS = 5\n",
        "emb_dim = 16\n",
        "latent_dim = 16\n",
        "num_layers = 1\n",
        "dropout = True"
      ],
      "metadata": {
        "id": "2oymOcBz_CF-"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  @title Image functions\n",
        "\n",
        "\n",
        "# Need to add loss curves\n",
        "\n",
        "# Image denormalisation\n",
        "#  ax.imshow(image.permute(1, 2, 0))"
      ],
      "metadata": {
        "id": "xAdaYhkFCPby"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Dataset loading/creation\n",
        "\n",
        "# Loading the dataset\n",
        "train_dataset = load_dataset(\"daniel3303/StoryReasoning\", split=\"train\")\n",
        "test_dataset = load_dataset(\"daniel3303/StoryReasoning\", split=\"test\")\n",
        "\n",
        "# Split the training dataset into training and validation sets\n",
        "train_size = int(0.8 * len(train_dataset))\n",
        "val_size = len(train_dataset) - train_size\n",
        "train_indices, val_indices = random_split(\n",
        "    range(len(train_dataset)),\n",
        "    [train_size, val_size]) # split the database universally here, avoids data leakage\n",
        "    # for a range of the len of train set, split into these indices\n",
        "\n",
        "# For the Sequence prediction task\n",
        "tokenizer = BertTokenizer.from_pretrained(\"google-bert/bert-base-uncased\",  padding=True, truncation=True)\n",
        "sp_train_dataset = SequencePredictionDataset(train_dataset, tokenizer) # Instantiate the train dataset\n",
        "sp_test_dataset = SequencePredictionDataset(test_dataset, tokenizer) # Instantiate the test dataset\n",
        "\n",
        "# Instantiate the dataloaders\n",
        "sp_train_subset = Subset(sp_train_dataset, train_indices)\n",
        "train_dataloader = DataLoader(sp_train_subset, batch_size=8, shuffle=True)\n",
        "# We will use the validation set to visualize the progress.\n",
        "sp_val_subset = Subset(sp_train_dataset, val_indices)\n",
        "val_dataloader = DataLoader(sp_val_subset, batch_size=4, shuffle=False)\n",
        "\n",
        "test_dataloader = DataLoader(sp_test_dataset, batch_size=4, shuffle=False)\n",
        "\n",
        "# Text dataset\n",
        "tokenizer = BertTokenizer.from_pretrained(\"google-bert/bert-base-uncased\",  padding=True, truncation=True)\n",
        "text_dataset = TextTaskDataset(train_dataset)\n",
        "text_test_dataset = TextTaskDataset(test_dataset)\n",
        "\n",
        "# Universal split applied HERE (use the same indices)\n",
        "text_train_subset = Subset(text_dataset, train_indices)\n",
        "text_val_subset   = Subset(text_dataset, val_indices)\n",
        "\n",
        "text_train_dataloader = DataLoader(text_train_subset, batch_size=4, shuffle=True)\n",
        "text_val_dataloader   = DataLoader(text_val_subset, batch_size=4, shuffle=False)\n",
        "text_test_dataloader  = DataLoader(text_test_dataset, batch_size=4, shuffle=False)\n",
        "\n",
        "# Image dataset\n",
        "autoencoder_train_dataset = AutoEncoderTaskDataset(train_dataset)\n",
        "autoencoder_test_dataset = AutoEncoderTaskDataset(test_dataset)\n",
        "ae_train_subset = Subset(autoencoder_train_dataset, train_indices)\n",
        "ae_val_subset   = Subset(autoencoder_train_dataset, val_indices)\n",
        "\n",
        "autoencoder_train_dataloader = DataLoader(ae_train_subset, batch_size=4, shuffle=True,\n",
        "                                    collate_fn=lambda batch: torch.stack(batch, dim=0))\n",
        "autoencoder_test_dataloader = DataLoader(autoencoder_test_dataset, batch_size=4, shuffle=False,\n",
        "                                    collate_fn=lambda batch: torch.stack(batch, dim=0))\n",
        "autoencoder_val_dataloader = DataLoader(ae_val_subset, batch_size=4, shuffle=False,\n",
        "                                    collate_fn=lambda batch: torch.stack(batch, dim=0))"
      ],
      "metadata": {
        "id": "vapVEXWR9wz6"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Initialising models and setup\n",
        "\n",
        "# Initializing the NLP models\n",
        "encoder = EncoderLSTM(tokenizer.vocab_size, emb_dim, latent_dim, num_layers, dropout).to(device)\n",
        "decoder = DecoderLSTM(tokenizer.vocab_size, emb_dim, latent_dim, num_layers, dropout).to(device)\n",
        "text_autoencoder = Seq2SeqLSTM(encoder, decoder).to(device)\n",
        "text_autoencoder, _, _, _ = load_checkpoint_from_drive(text_autoencoder, None, filename='text_autoencoder.pth')\n",
        "\n",
        "total_params = sum(p.numel() for p in text_autoencoder.parameters())\n",
        "print(f\"Total parameters (Not trainable): {total_params}\")\n",
        "# Deactivating training from this model for efficiency\n",
        "for param in text_autoencoder.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "\n",
        "# Initializing visual models\n",
        "visual_autoencoder = VisualAutoencoder(latent_dim=16).to(device)\n",
        "# visual_autoencoder.apply(init_weights)\n",
        "\n",
        "# IMPORTANT: Load checkpoint WITH optimizer because we are continuing training\n",
        "visual_ae_optimizer = torch.optim.Adam(\n",
        "    (p for p in visual_autoencoder.parameters() if p.requires_grad),\n",
        "    lr=1e-4\n",
        ")\n",
        "\n",
        "visual_autoencoder, _, epoch_loaded, loss_loaded = load_checkpoint_from_drive(\n",
        "    visual_autoencoder,\n",
        "    None,\n",
        "    filename=\"visual_autoencoder.pth\"\n",
        ")\n",
        "\n",
        "total_params = sum(p.numel() for p in visual_autoencoder.parameters() if p.requires_grad)\n",
        "print(f\"Total trainable parameters in visual autoencoder: {total_params}\")\n",
        "print(f\"Visual Autoencoder loaded from epoch {epoch_loaded} with loss {loss_loaded:.4f}\")\n",
        "\n",
        "\n",
        "# We put all the sizes the same, not ideal as well\n",
        "sequence_predictor = SequencePredictor(\n",
        "    visual_autoencoder,\n",
        "    text_autoencoder,\n",
        "    latent_dim,\n",
        "    latent_dim\n",
        ").to(device)\n",
        "\n",
        "total_params = sum(p.numel() for p in sequence_predictor.parameters() if p.requires_grad)\n",
        "print(f\"Total trainable parameters in the whole model: {total_params}\")\n",
        "\n",
        "total_params = sum(p.numel() for p in sequence_predictor.parameters())\n",
        "print(f\"Total parameters: {total_params}\")\n",
        "\n",
        "\n",
        "# Training tools\n",
        "criterion_images = nn.L1Loss()\n",
        "criterion_ctx = nn.MSELoss()\n",
        "criterion_text = nn.CrossEntropyLoss(ignore_index=tokenizer.convert_tokens_to_ids(tokenizer.pad_token))\n",
        "\n",
        "# Optimizer for SEQUENCE PREDICTOR created AFTER model definition\n",
        "optimizer = torch.optim.Adam(\n",
        "    (p for p in sequence_predictor.parameters() if p.requires_grad),\n",
        "    lr=0.001\n",
        ")\n"
      ],
      "metadata": {
        "id": "oAA5xqNrCksM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31d9c967-85e8-465f-f2e0-6b1426e3007f"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checkpoint loaded from: /content/gdrive/MyDrive/DL_Checkpoints/text_autoencoder.pth (epoch 15)\n",
            "Total parameters (Not trainable): 1499930\n",
            "Checkpoint loaded from: /content/gdrive/MyDrive/DL_Checkpoints/visual_autoencoder.pth (epoch 0)\n",
            "Total trainable parameters in visual autoencoder: 503539\n",
            "Visual Autoencoder loaded from epoch 0 with loss 0.0870\n",
            "Total trainable parameters in the whole model: 507028\n",
            "Total parameters: 2006958\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "mK8GHVvb-0V5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "outputId": "9d3b6c4c-834e-4814-ec62-87883c1ce8aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation on training dataset\n",
            "----------------\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'sequence_predictor' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4035241.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN_EPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     metrics = train_sequence_predictor(\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msequence_predictor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/dnnls_final_project/src/train/train_sequence_predictor.py\u001b[0m in \u001b[0;36mtrain_sequence_predictor\u001b[0;34m(model, train_dataloader, val_dataloader, optimizer, criterion_images, criterion_ctx, criterion_text, tokenizer, device, num_epochs)\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mloss_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction_flat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_flat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0;31m# Combining the losses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_im\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss_text\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mloss_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m             \u001b[0;31m# Optimizing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'sequence_predictor' is not defined"
          ]
        }
      ],
      "source": [
        "# @title Sequence predictor training sequence\n",
        "losses = []\n",
        "all_epoch_losses = []\n",
        "all_train_mse = []\n",
        "all_train_perplexity = []\n",
        "all_train_bleu = []\n",
        "all_train_crossmodal = []\n",
        "all_train_ssim = []\n",
        "\n",
        "all_val_mse = []\n",
        "all_val_perplexity = []\n",
        "all_val_bleu = []\n",
        "all_val_crossmodal = []\n",
        "all_val_ssim = []\n",
        "\n",
        "\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "\n",
        "    metrics = train_sequence_predictor(\n",
        "        model=sequence_predictor,\n",
        "        train_dataloader=train_dataloader,\n",
        "        val_dataloader=val_dataloader,\n",
        "        optimizer=optimizer,\n",
        "        criterion_images=criterion_images,\n",
        "        criterion_ctx=criterion_ctx,\n",
        "        criterion_text=criterion_text,\n",
        "        tokenizer=tokenizer,\n",
        "        device=device,\n",
        "        num_epochs=1\n",
        "    )\n",
        "\n",
        "    all_epoch_losses.append(metrics[\"epoch_losses\"][0])\n",
        "\n",
        "    all_train_mse.append(metrics[\"train_mse\"][0])\n",
        "    all_train_perplexity.append(metrics[\"train_perplexity\"][0])\n",
        "    all_train_bleu.append(metrics[\"train_bleu\"][0])\n",
        "    all_train_crossmodal.append(metrics[\"train_crossmodal\"][0])\n",
        "    all_train_ssim.append(metrics[\"train_ssim\"][0])\n",
        "\n",
        "    all_val_mse.append(metrics[\"val_mse\"][0])\n",
        "    all_val_perplexity.append(metrics[\"val_perplexity\"][0])\n",
        "    all_val_bleu.append(metrics[\"val_bleu\"][0])\n",
        "    all_val_crossmodal.append(metrics[\"val_crossmodal\"][0])\n",
        "    all_val_ssim.append(metrics[\"val_ssim\"][0])\n",
        "\n",
        "    if epoch % 5 == 0:\n",
        "        save_checkpoint_to_drive(\n",
        "            sequence_predictor,\n",
        "            optimizer,\n",
        "            epoch,\n",
        "            all_epoch_losses[-1],\n",
        "            filename=f\"sequence_predictor.pth\"\n",
        "        )\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Image autoencoder training\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "\n",
        "    epoch_losses = train_visual_autoencoder(\n",
        "        model=visual_autoencoder,\n",
        "        dataloader=autoencoder_train_dataloader,\n",
        "        optimizer=visual_ae_optimizer,\n",
        "        criterion=criterion_images,\n",
        "        device=device,\n",
        "        num_epochs=1\n",
        "    )\n",
        "\n",
        "    if epoch % 5 == 0:\n",
        "        save_checkpoint_to_drive(\n",
        "            visual_autoencoder,\n",
        "            visual_ae_optimizer,\n",
        "            epoch,\n",
        "            epoch_losses[-1],\n",
        "            filename=f\"visual_autoencoder.pth\"\n",
        "        )\n"
      ],
      "metadata": {
        "id": "iQ8TruiLHdCW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title SP METRIC PLOTTING\n",
        "\n",
        "# Training Loss Curve\n",
        "plt.plot(all_epoch_losses)\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Training Loss Over Time\")\n",
        "plt.show()\n",
        "\n",
        "# Training SSIM\n",
        "plt.plot(all_train_ssim)\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"SSIM\")\n",
        "plt.title(\"Training SSIM Over Epochs\")\n",
        "plt.show()\n",
        "\n",
        "# Training BLEU\n",
        "plt.plot(all_train_bleu)\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"BLEU\")\n",
        "plt.title(\"Training BLEU Over Epochs\")\n",
        "plt.show()\n",
        "\n",
        "# Training MSE\n",
        "plt.plot(all_train_mse)\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"MSE\")\n",
        "plt.title(\"Training MSE Over Epochs\")\n",
        "plt.show()\n",
        "\n",
        "# Training Perplexity\n",
        "plt.plot(all_train_perplexity)\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Perplexity\")\n",
        "plt.title(\"Training Perplexity Over Epochs\")\n",
        "plt.show()\n",
        "\n",
        "# Training Cross-Modal Similarity\n",
        "plt.plot(all_train_crossmodal)\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Cross-modal similarity\")\n",
        "plt.title(\"Training Cross-modal Similarity Over Epochs\")\n",
        "plt.show()\n",
        "\n",
        "# Validation SSIM\n",
        "plt.plot(all_val_ssim)\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"SSIM\")\n",
        "plt.title(\"Validation SSIM Over Epochs\")\n",
        "plt.show()\n",
        "\n",
        "# Validation BLEU\n",
        "plt.plot(all_val_bleu)\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"BLEU\")\n",
        "plt.title(\"Validation BLEU Over Epochs\")\n",
        "plt.show()\n",
        "\n",
        "# Validation MSE\n",
        "plt.plot(all_val_mse)\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"MSE\")\n",
        "plt.title(\"Validation MSE Over Epochs\")\n",
        "plt.show()\n",
        "\n",
        "# Validation Perplexity\n",
        "plt.plot(all_val_perplexity)\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Perplexity\")\n",
        "plt.title(\"Validation Perplexity Over Epochs\")\n",
        "plt.show()\n",
        "\n",
        "# Validation Cross-Modal Similarity\n",
        "plt.plot(all_val_crossmodal)\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Cross-modal similarity\")\n",
        "plt.title(\"Validation Cross-modal Similarity Over Epochs\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "DxtcWKuPAeXV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}