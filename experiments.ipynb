{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5tIaBJjvVER9"
      },
      "outputs": [],
      "source": [
        "rm -rf /content/dnnls_final_project\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6jLHj0ncUz3v"
      },
      "outputs": [],
      "source": [
        "%cd /content\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IFHu0dtz_Z81"
      },
      "outputs": [],
      "source": [
        "\n",
        "!git clone https://github.com/scrannah/dnnls_final_project.git\n",
        "%cd /content/dnnls_final_project\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gZzbOwYQUYM_"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "SGQbQHWcEwVg"
      },
      "outputs": [],
      "source": [
        "# @title Importing the necessary libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "from datasets.fingerprint import random\n",
        "from torch.utils.data import Dataset, DataLoader, random_split, Subset\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision.transforms.functional as FT\n",
        "import torch.nn.functional as F\n",
        "from torchmetrics.image import StructuralSimilarityIndexMeasure\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "\n",
        "from transformers import BertTokenizer\n",
        "import gc\n",
        "\n",
        "import textwrap\n",
        "import sys\n",
        "import os\n",
        "\n",
        "abspath = r\".\\dnnls_final_project\"\n",
        "sys.path.append(os.path.abspath(abspath ))\n",
        "\n",
        "# put src imports here models training utils etc\n",
        "from src.attention.attention import Attention\n",
        "\n",
        "from src.dataloaders.sp_dataset import SequencePredictionDataset\n",
        "from src.dataloaders.tag_extraction import parse_gdi_text\n",
        "from src.dataloaders.text_dataset import TextTaskDataset\n",
        "from src.dataloaders.vae_dataset import AutoEncoderTaskDataset\n",
        "\n",
        "# from src.encoders.visual_autoencoder import Backbone, VisualEncoder, VisualDecoder, VisualAutoencoder\n",
        "from src.encoders.text_autoencoder import EncoderLSTM, DecoderLSTM, Seq2SeqLSTM\n",
        "from src.encoders.perceptual_loss import PerceptualLoss\n",
        "# from src.encoders.unetvisual_autoencoder import UNetBackbone, UNetVisualEncoder, UNetVisualDecoder, UNetVisualAutoencoder\n",
        "# from src.encoders.VAEvisual_autoencoder import VAEBackbone, VAEVisualEncoder, VAEVisualDecoder, VAEVisualAutoencoder\n",
        "# from src.encoders.Newvisual_autoencoder import NewBackbone, NewVisualEncoder, NewVisualDecoder, NewVisualAutoencoder\n",
        "from src.encoders.resnet18_visualautoencoder import ResNet18Backbone, NewVisualEncoder, NewVisualDecoder, NewVisualAutoencoder\n",
        "from src.encoders.gradient_loss import sobel_gradients, sobel_gradient_loss\n",
        "\n",
        "# from src.model.sequence_predictor import SequencePredictor\n",
        "from src.model.cmsequence_predictor import CMSequencePredictor\n",
        "\n",
        "from src.train.train_sequence_predictor import train_sequence_predictor\n",
        "from src.train.train_visual_autoencoder import train_visual_autoencoder\n",
        "\n",
        "from src.utils.checkpoints import save_checkpoint_to_drive, load_checkpoint_from_drive\n",
        "from src.utils.training_utils import init_weights, validation, show_image\n",
        "from src.utils.token_generate import generate\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2oymOcBz_CF-"
      },
      "outputs": [],
      "source": [
        "# @title Variables and initial setup\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "N_EPOCHS = 10\n",
        "emb_dim = 16\n",
        "latent_dim = 16 # Changed from 256 to 16 to match the checkpoint\n",
        "num_layers = 1\n",
        "dropout = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vapVEXWR9wz6"
      },
      "outputs": [],
      "source": [
        "# @title Dataset loading/creation\n",
        "\n",
        "# Loading the dataset\n",
        "train_dataset = load_dataset(\"daniel3303/StoryReasoning\", split=\"train\")\n",
        "test_dataset = load_dataset(\"daniel3303/StoryReasoning\", split=\"test\")\n",
        "\n",
        "# Split the training dataset into training and validation sets\n",
        "train_size = int(0.8 * len(train_dataset))\n",
        "val_size = len(train_dataset) - train_size\n",
        "train_indices, val_indices = random_split(\n",
        "    range(len(train_dataset)),\n",
        "    [train_size, val_size]) # split the database universally here, avoids data leakage\n",
        "    # for a range of the len of train set, split into these indices\n",
        "\n",
        "# For the Sequence prediction task\n",
        "tokenizer = BertTokenizer.from_pretrained(\"google-bert/bert-base-uncased\",  padding=True, truncation=True)\n",
        "sp_train_dataset = SequencePredictionDataset(train_dataset, tokenizer) # Instantiate the train dataset\n",
        "sp_test_dataset = SequencePredictionDataset(test_dataset, tokenizer) # Instantiate the test dataset\n",
        "\n",
        "# Instantiate the dataloaders\n",
        "sp_train_subset = Subset(sp_train_dataset, train_indices)\n",
        "train_dataloader = DataLoader(sp_train_subset, batch_size=8, shuffle=True)\n",
        "# We will use the validation set to visualize the progress.\n",
        "sp_val_subset = Subset(sp_train_dataset, val_indices)\n",
        "val_dataloader = DataLoader(sp_val_subset, batch_size=4, shuffle=True)\n",
        "\n",
        "test_dataloader = DataLoader(sp_test_dataset, batch_size=4, shuffle=True)\n",
        "\n",
        "# Text dataset\n",
        "tokenizer = BertTokenizer.from_pretrained(\"google-bert/bert-base-uncased\",  padding=True, truncation=True)\n",
        "text_dataset = TextTaskDataset(train_dataset)\n",
        "text_test_dataset = TextTaskDataset(test_dataset)\n",
        "\n",
        "# Universal split applied HERE (use the same indices)\n",
        "text_train_subset = Subset(text_dataset, train_indices)\n",
        "text_val_subset   = Subset(text_dataset, val_indices)\n",
        "\n",
        "text_train_dataloader = DataLoader(text_train_subset, batch_size=4, shuffle=True)\n",
        "text_val_dataloader   = DataLoader(text_val_subset, batch_size=4, shuffle=False)\n",
        "text_test_dataloader  = DataLoader(text_test_dataset, batch_size=4, shuffle=False)\n",
        "\n",
        "# Image dataset\n",
        "autoencoder_train_dataset = AutoEncoderTaskDataset(train_dataset)\n",
        "autoencoder_test_dataset = AutoEncoderTaskDataset(test_dataset)\n",
        "ae_train_subset = Subset(autoencoder_train_dataset, train_indices)\n",
        "ae_val_subset   = Subset(autoencoder_train_dataset, val_indices)\n",
        "\n",
        "autoencoder_train_dataloader = DataLoader(ae_train_subset, batch_size=4, shuffle=True,\n",
        "                                    collate_fn=lambda batch: torch.stack(batch, dim=0))\n",
        "autoencoder_test_dataloader = DataLoader(autoencoder_test_dataset, batch_size=4, shuffle=False,\n",
        "                                    collate_fn=lambda batch: torch.stack(batch, dim=0))\n",
        "autoencoder_val_dataloader = DataLoader(ae_val_subset, batch_size=4, shuffle=False,\n",
        "                                    collate_fn=lambda batch: torch.stack(batch, dim=0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oAA5xqNrCksM"
      },
      "outputs": [],
      "source": [
        "from src.encoders.perceptual_loss import PerceptualLoss\n",
        "# @title Initialising models and setup\n",
        "\n",
        "# Initializing the NLP models\n",
        "encoder = EncoderLSTM(tokenizer.vocab_size, emb_dim, latent_dim, num_layers, dropout).to(device)\n",
        "decoder = DecoderLSTM(tokenizer.vocab_size, emb_dim, latent_dim, num_layers, dropout).to(device)\n",
        "text_autoencoder = Seq2SeqLSTM(encoder, decoder).to(device)\n",
        "text_autoencoder, _, epoch_loaded, loss_loaded = load_checkpoint_from_drive(text_autoencoder, None, filename='text_autoencoder.pth') # remember to load correct one\n",
        "\n",
        "total_params = sum(p.numel() for p in text_autoencoder.parameters())\n",
        "print(f\"Total parameters (Not trainable): {total_params}\")\n",
        "print(f\"Text Autoencoder loaded from epoch {epoch_loaded} with loss {loss_loaded:.4f}\")\n",
        "# Deactivating training from this model for efficiency\n",
        "#for param in text_autoencoder.parameters():\n",
        "    #param.requires_grad = False\n",
        "\n",
        "\n",
        "# Initializing visual models\n",
        "# visual_autoencoder = VisualAutoencoder(latent_dim=latent_dim).to(device)\n",
        "visual_autoencoder = VisualAutoencoder(latent_dim=latent_dim).to(device)\n",
        "visual_autoencoder.apply(init_weights)\n",
        "\n",
        "# Load checkpoint WITH optimizer if continuing training exactly as before\n",
        "#comment out when freezing ae\n",
        "visual_ae_optimizer = torch.optim.Adam(\n",
        "    (p for p in visual_autoencoder.parameters() if p.requires_grad),\n",
        "    lr=1e-2\n",
        ")\n",
        "\n",
        "\n",
        "visual_autoencoder, _, epoch_loaded, loss_loaded = load_checkpoint_from_drive(\n",
        "  visual_autoencoder,   None,\n",
        "    filename=\"Newvisual_autoencoder128244244resnet.pth\"\n",
        ")\n",
        "\n",
        "for p in visual_autoencoder.parameters(): # freeze for sequence prediction tasks\n",
        "    p.requires_grad = False\n",
        "\n",
        "total_params = sum(p.numel() for p in visual_autoencoder.parameters() if p.requires_grad)\n",
        "print(f\"Total trainable parameters in visual autoencoder: {total_params}\")\n",
        "#print(f\"Visual Autoencoder loaded from epoch {epoch_loaded} with loss {loss_loaded:.4f}\")\n",
        "\n",
        "\n",
        "sequence_predictor = CMSequencePredictor(\n",
        "    visual_autoencoder,\n",
        "    text_autoencoder,\n",
        "    latent_dim,latent_dim).to(device)\n",
        "\n",
        "sequence_predictor, _, epoch_loaded, loss_loaded = load_checkpoint_from_drive(\n",
        "   sequence_predictor,   None,\n",
        "    filename=\"sequence_predictorFINAL.pth\"\n",
        "#)\n",
        "total_params = sum(p.numel() for p in sequence_predictor.parameters())\n",
        "print(f\"Total parameters: {total_params}\")\n",
        "#print(f\"Sequence Prediction loaded from epoch {epoch_loaded} with loss {loss_loaded:.4f}\")\n",
        "\n",
        "\n",
        "\n",
        "# Training tools\n",
        "criterion_images = nn.L1Loss()\n",
        "criterion_ctx = nn.MSELoss()\n",
        "criterion_text = nn.CrossEntropyLoss(ignore_index=tokenizer.convert_tokens_to_ids(tokenizer.pad_token))\n",
        "criterion_percep = PerceptualLoss(device).to(device)\n",
        "\n",
        "# Optimizer for SEQUENCE PREDICTOR created AFTER model definition\n",
        "optimizer = torch.optim.Adam(\n",
        "    (p for p in sequence_predictor.parameters() if p.requires_grad),\n",
        "    lr=0.001\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "mK8GHVvb-0V5"
      },
      "outputs": [],
      "source": [
        "# @title Sequence predictor training sequence\n",
        "losses = []\n",
        "all_epoch_losses = []\n",
        "all_train_mse = []\n",
        "all_train_perplexity = []\n",
        "all_train_bleu = []\n",
        "all_train_crossmodal = []\n",
        "all_train_ssim = []\n",
        "\n",
        "all_val_mse = []\n",
        "all_val_perplexity = []\n",
        "all_val_bleu = []\n",
        "all_val_crossmodal = []\n",
        "all_val_ssim = []\n",
        "\n",
        "lambda_cm = 0.01\n",
        "N_EPOCHS = 21\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "\n",
        "    metrics = train_sequence_predictor(\n",
        "        model=sequence_predictor,\n",
        "        train_dataloader=train_dataloader,\n",
        "        val_dataloader=val_dataloader,\n",
        "        optimizer=optimizer,\n",
        "        criterion_images=criterion_images,\n",
        "        criterion_ctx=criterion_ctx,\n",
        "        criterion_text=criterion_text,\n",
        "        tokenizer=tokenizer,\n",
        "        device=device,\n",
        "        num_epochs=1,\n",
        "        lambda_cm=lambda_cm\n",
        "    )\n",
        "\n",
        "    all_epoch_losses.append(metrics[\"epoch_losses\"][0])\n",
        "\n",
        "    all_train_mse.append(metrics[\"train_mse\"][0])\n",
        "    all_train_perplexity.append(metrics[\"train_perplexity\"][0])\n",
        "    all_train_bleu.append(metrics[\"train_bleu\"][0])\n",
        "    all_train_crossmodal.append(metrics[\"train_crossmodal\"][0])\n",
        "    all_train_ssim.append(metrics[\"train_ssim\"][0])\n",
        "\n",
        "    all_val_mse.append(metrics[\"val_mse\"][0])\n",
        "    all_val_perplexity.append(metrics[\"val_perplexity\"][0])\n",
        "    all_val_bleu.append(metrics[\"val_bleu\"][0])\n",
        "    all_val_crossmodal.append(metrics[\"val_crossmodal\"][0])\n",
        "    all_val_ssim.append(metrics[\"val_ssim\"][0])\n",
        "\n",
        "    if epoch % 5 == 0:\n",
        "        save_checkpoint_to_drive(\n",
        "            sequence_predictor,\n",
        "            optimizer,\n",
        "            epoch,\n",
        "            all_epoch_losses[-1],\n",
        "            filename=f\"sequence_predictor.pth\"\n",
        "        )\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b8a9c304"
      },
      "source": [
        "N_EPOCHS = 21\n",
        "lambda_cm = 0.01\n",
        "for epoch in range(N_EPOCHS):\n",
        "  with torch.no_grad():\n",
        "    sequence_predictor.eval()\n",
        "    running_loss = 0.0 # This line was over-indented\n",
        "    for frames, descriptions, image_target, text_target in test_dataloader:\n",
        "        # Send images and tokens to the GPU\n",
        "        descriptions = descriptions.to(device)\n",
        "        frames = frames.to(device)\n",
        "        image_target = image_target.to(device)\n",
        "        text_target = text_target.to(device)\n",
        "        # Predictions from our model\n",
        "        pred_image_content, pred_image_context, predicted_text_logits_k, _, _, z_t_flat, z_v_flat = sequence_predictor(frames,\n",
        "                                                                                                        descriptions,\n",
        "                                                                                                        text_target)\n",
        "        # Computing losses\n",
        "        # Loss for image reconstruction\n",
        "        loss_im = criterion_images(pred_image_content, image_target)  # image loss\n",
        "        # Loss for the average pattern the images contain\n",
        "        mu_global = frames.mean(dim=[0, 1])\n",
        "        mu_global = mu_global.unsqueeze(0).expand_as(pred_image_context)\n",
        "        loss_context = criterion_ctx(pred_image_context, mu_global)  # context loss\n",
        "        # Loss function for the text prediction\n",
        "        prediction_flat = predicted_text_logits_k.reshape(-1, tokenizer.vocab_size)\n",
        "        target_labels = text_target.squeeze(1)[:, 1:]  # Slice to get [8, 119]\n",
        "        target_flat = target_labels.reshape(-1)\n",
        "        loss_text = criterion_text(prediction_flat, target_flat)\n",
        "        loss_align = 1 - F.cosine_similarity(z_v_flat, z_t_flat, dim=1).mean()\n",
        "        # Combining the losses\n",
        "        loss = loss_im + loss_text + 0.2 * loss_context + lambda_cm * loss_align\n",
        "\n",
        "        running_loss += loss.item() * frames.size(0)\n",
        "\n",
        "    epoch_loss = running_loss / len(test_dataloader.dataset)\n",
        "    print(f\"Epoch {epoch+1} Test Combined Loss: {epoch_loss:.4f}\")\n",
        "\n",
        "    test_metrics = validation(\n",
        "        model=sequence_predictor,\n",
        "        data_loader=test_dataloader,\n",
        "        device= device,\n",
        "        tokenizer=tokenizer,\n",
        "        criterion_text=criterion_text,\n",
        "        criterion_ctx=criterion_ctx\n",
        "    )\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HB7L3e1x7gYJ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v77qqnb3BLOY"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "collapsed": true,
        "id": "qOTHrsOU4-px"
      },
      "outputs": [],
      "source": [
        "# @title Computing and showing average images\n",
        "N = 1000\n",
        "H, W = 60, 125\n",
        "\n",
        "# Tensors to accumulate sum (for mean) and sum of squares (for variance)\n",
        "avg_images = [torch.zeros((3, H, W)) for _ in range(5)]\n",
        "sum_sq_diff = [torch.zeros((3, H, W)) for _ in range(5)] # Placeholder for variance numerator\n",
        "transform = transforms.Compose([\n",
        "  transforms.Resize((H, W)), # Add this line to resize images\n",
        "  transforms.ToTensor()\n",
        "])\n",
        "\n",
        "# --- First Pass: Calculate the Sum (for Mean) ---\n",
        "print(\"Starting Pass 1: Calculating Mean...\")\n",
        "\n",
        "for i in range(N):\n",
        "    # Process sequence i\n",
        "    sequence = train_dataset[i][\"images\"]\n",
        "\n",
        "    for j in range(5):\n",
        "        image = transform(sequence[j])\n",
        "        avg_images[j] += image # Sum for mean\n",
        "\n",
        "# Final step for mean\n",
        "for j in range(5):\n",
        "    avg_images[j] /= N\n",
        "\n",
        "print(\"Starting Pass 2: Calculating Variance...\")\n",
        "\n",
        "for i in range(N):\n",
        "    # Process sequence i\n",
        "    sequence = train_dataset[i][\"images\"]\n",
        "\n",
        "    for j in range(5):\n",
        "        image = transform(sequence[j])\n",
        "\n",
        "        # Calculate (Image - Mean)^2\n",
        "        # Note: We detach the mean from the computation graph if it were being trained,\n",
        "        # but here we're just using it as a fixed statistical value.\n",
        "        diff = image - avg_images[j]\n",
        "        sum_sq_diff[j] += diff * diff # Element-wise squaring\n",
        "\n",
        "# --- Final step for Standard Deviation ---\n",
        "std_images = []\n",
        "for j in range(5):\n",
        "    # Variance = Sum of Squared Differences / N\n",
        "    variance = sum_sq_diff[j] / N\n",
        "\n",
        "    # Standard Deviation = sqrt(Variance)\n",
        "    std_dev = torch.sqrt(variance)\n",
        "    std_images.append(std_dev)\n",
        "\n",
        "print(\"Computation Complete. std_images is a list of 5 tensors (3x60x125).\")\n",
        "# You now have the 5 tensors you need for normalization (mean and std).\n",
        "\n",
        "fig, ax = plt.subplots(1,5, figsize=(15,5))\n",
        "for i in range(5):\n",
        "  avg_image = avg_images[i]\n",
        "\n",
        "  # Printing range of avg_image\n",
        "  print(torch.min(avg_image), torch.max(avg_image))\n",
        "\n",
        "  avg_imagen = (avg_image - torch.min(avg_image))/(torch.max(avg_image) - torch.min(avg_image))\n",
        "  show_image(ax[i], avg_imagen)\n",
        "\n",
        "# Create a matrix of images with the differences between avg_images\n",
        "fig, ax = plt.subplots(5,5, figsize=(15,8))\n",
        "\n",
        "for i in range(5):\n",
        "  for j in range(5):\n",
        "    if i == j:\n",
        "      avg_image = avg_images[i]\n",
        "      avg_imagen = (avg_image - torch.min(avg_image))/(torch.max(avg_image) - torch.min(avg_image))\n",
        "      show_image(ax[i,j], avg_imagen)\n",
        "    else:\n",
        "      diff = avg_images[i] - avg_images[j]\n",
        "      diff = (diff - torch.min(diff))/(torch.max(diff) - torch.min(diff))\n",
        "      show_image(ax[i,j], diff)\n",
        "    ax[i,j].set_xticks([])\n",
        "    ax[i,j].set_yticks([])\n",
        "plt.tight_layout()\n",
        "plt.subplots_adjust(\n",
        "    wspace=0, # Set horizontal space to zero\n",
        "    hspace=0  # Set vertical space to zero\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "iQ8TruiLHdCW"
      },
      "outputs": [],
      "source": [
        "# @title Image autoencoder training\n",
        "# batches_per_epoch = len(autoencoder_train_dataloader)\n",
        "\n",
        "# beta = 1.0\n",
        "# kl_anneal_epoch = batches_per_epoch * 20\n",
        "lambda_percep = 0.03\n",
        "lambda_ctx = 0.2\n",
        "lambda_grad = 0.005\n",
        "N_EPOCHS = 26\n",
        "# global_step = 0 # this did solve epoch counting but it will break training loop\n",
        "fixed_batch = next(iter(autoencoder_train_dataloader))\n",
        "fixed_batch = fixed_batch.to(device)\n",
        "for epoch in range(N_EPOCHS):\n",
        "\n",
        "    epoch_losses = train_visual_autoencoder(\n",
        "        model=visual_autoencoder,\n",
        "        train_dataloader=autoencoder_train_dataloader,\n",
        "        optimizer=visual_ae_optimizer,\n",
        "        criterion_images=criterion_images,\n",
        "        criterion_percep=criterion_percep,\n",
        "        criterion_ctx=criterion_ctx,\n",
        "        lambda_percep=lambda_percep,\n",
        "        lambda_ctx=lambda_ctx,\n",
        "        lambda_grad=lambda_grad,\n",
        "        device=device\n",
        "    )\n",
        "\n",
        "    if epoch % 5 == 0:\n",
        "        save_checkpoint_to_drive(\n",
        "            visual_autoencoder,\n",
        "            visual_ae_optimizer,\n",
        "            epoch,\n",
        "            epoch_losses[-1],\n",
        "            filename=f\"Newvisual_autoencoder128244244resnet.pth\"\n",
        "        )\n",
        "\n",
        "\n",
        "    if epoch % 1 == 0:   # after every epoch\n",
        "        with torch.no_grad():\n",
        "            visual_autoencoder.eval()\n",
        "\n",
        "            sample_batch = next(iter(autoencoder_train_dataloader))\n",
        "            sample_batch = sample_batch.to(device)\n",
        "\n",
        "            x_hat = visual_autoencoder(sample_batch)\n",
        "\n",
        "\n",
        "            x_content, x_context = x_hat  # assuming tuple\n",
        "            # print(\"x_content:\", x_content.shape)   # want (B, 3, H, W)\n",
        "            # print(\"x_context:\", x_context.shape)\n",
        "            z = visual_autoencoder.encoder(fixed_batch)\n",
        "            fixedx_content, fixedx_context = visual_autoencoder(fixed_batch)\n",
        "\n",
        "\n",
        "\n",
        "            print(\"img mean/std:\",\n",
        "                  fixed_batch.mean().item(), fixed_batch.std().item(),\n",
        "                  \"min/max:\",\n",
        "                  fixed_batch.min().item(), fixed_batch.max().item())\n",
        "\n",
        "            print(\"out mean/std:\",\n",
        "                  fixedx_content.mean().item(), fixedx_content.std().item(),\n",
        "                  \"min/max:\",\n",
        "                  fixedx_content.min().item(), fixedx_content.max().item())\n",
        "\n",
        "            print(\"z mean/std:\",\n",
        "                  z.mean().item(), z.std().item())\n",
        "\n",
        "\n",
        "            img_mean = torch.tensor([0.485, 0.456, 0.406])   #image norms if i need them\n",
        "            img_std  = torch.tensor([0.229, 0.224, 0.225])\n",
        "\n",
        "            fig, axs = plt.subplots(1, 3, figsize=(12, 4))\n",
        "\n",
        "\n",
        "            show_image(axs[0], sample_batch[0].cpu(), de_normalize=False, img_mean=img_mean, img_std=img_std)\n",
        "            axs[0].set_title(\"Original\")\n",
        "\n",
        "            show_image(axs[1], x_hat[0][0].cpu(), de_normalize=False, img_mean=img_mean, img_std=img_std)\n",
        "            axs[1].set_title(\"Content\")\n",
        "\n",
        "            show_image(axs[2], x_hat[0][1].cpu(), de_normalize=False, img_mean=img_mean, img_std=img_std)\n",
        "            axs[2].set_title(\"Context\")\n",
        "\n",
        "\n",
        "\n",
        "            for a in axs:\n",
        "                a.axis(\"off\")\n",
        "\n",
        "            plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DxtcWKuPAeXV"
      },
      "outputs": [],
      "source": [
        "# @title SP METRIC PLOTTING\n",
        "\n",
        "# Training Loss Curve\n",
        "plt.plot(all_epoch_losses)\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Testing Loss Over Time\")\n",
        "plt.show()\n",
        "\n",
        "# Training SSIM\n",
        "plt.plot(all_test_ssim)\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"SSIM\")\n",
        "plt.title(\"Testing SSIM Over Epochs\")\n",
        "plt.show()\n",
        "\n",
        "# Training BLEU\n",
        "plt.plot(all_test_bleu)\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"BLEU\")\n",
        "plt.title(\"Testing BLEU Over Epochs\")\n",
        "plt.show()\n",
        "\n",
        "# Training L1\n",
        "plt.plot(all_test_mse)\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"MSE\")\n",
        "plt.title(\"Testing MSE Over Epochs\")\n",
        "plt.show()\n",
        "\n",
        "# Training Perplexity\n",
        "plt.plot(all_test_perplexity)\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Perplexity\")\n",
        "plt.title(\"Testing Perplexity Over Epochs\")\n",
        "plt.show()\n",
        "\n",
        "# Training Cross-Modal Similarity\n",
        "plt.plot(all_test_crossmodal)\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Cross-modal similarity\")\n",
        "plt.title(\"Testing Cross-modal Similarity Over Epochs\")\n",
        "plt.show()\n",
        "\n",
        "# Validation SSIM\n",
        "plt.plot(all_val_ssim)\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"SSIM\")\n",
        "plt.title(\"Validation SSIM Over Epochs\")\n",
        "plt.show()\n",
        "\n",
        "# Validation BLEU\n",
        "plt.plot(all_val_bleu)\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"BLEU\")\n",
        "plt.title(\"Validation BLEU Over Epochs\")\n",
        "plt.show()\n",
        "\n",
        "# Validation L1\n",
        "plt.plot(all_val_mse)\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"MSE\")\n",
        "plt.title(\"Validation MSE Over Epochs\")\n",
        "plt.show()\n",
        "\n",
        "# Validation Perplexity\n",
        "plt.plot(all_val_perplexity)\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Perplexity\")\n",
        "plt.title(\"Validation Perplexity Over Epochs\")\n",
        "plt.show()\n",
        "\n",
        "# Validation Cross-Modal Similarity\n",
        "plt.plot(all_val_crossmodal)\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Cross-modal similarity\")\n",
        "plt.title(\"Validation Cross-modal Similarity Over Epochs\")\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}